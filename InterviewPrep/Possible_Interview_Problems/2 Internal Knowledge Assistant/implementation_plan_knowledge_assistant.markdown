# Implementation Plan for Internal Knowledge Assistant

This document provides a detailed implementation plan for the Internal Knowledge Assistant system, covering each component, the tools or GCP services used, and the reasoning behind their selection. The system integrates Confluence, Google Drive, and Slack, offering semantic search with Retrieval-Augmented Generation (RAG), secure user authentication with role-based access control (RBAC), multi-source document indexing, temporal relevance, and leverages a Foundational Model for natural language processing.

---

## System Overview

The Internal Knowledge Assistant enables employees to search across Confluence, Google Drive, and Slack using semantic search and natural language responses. Key requirements include:
1. **RAG with Vector DBs**: Semantic search using embeddings and augment with a Foundational Model for natural language answers.
2. **User Authentication and RBAC**: Secure authentication and role-based access to documents.
3. **Multi-Source Document Indexing**: Ingest and index content from Confluence, Google Drive, and Slack.
4. **Temporal Relevance**: Prioritize recent information in search results.

The system uses **Angular** for the frontend, **FastAPI** for the backend, and **GCP** for infrastructure, with a Foundational Model hosted on Vertex AI.

---

## Component 1: Angular Frontend (Web App)

### Purpose
The Angular frontend provides a user-friendly interface for employees to log in, search, and view results, including document snippets, metadata, and answers generated by the Foundational Model.

### Implementation Plan
1. **Set Up Angular Project**:
   - **Tool**: Angular CLI
   - **Steps**:
     - Install Node.js and Angular CLI: `npm install -g @angular/cli`.
     - Create a new project: `ng new knowledge-assistant-frontend`.
     - Navigate to the project directory: `cd knowledge-assistant-frontend`.
   - **Why**: Angular CLI simplifies project setup, dependency management, and build processes. Angular’s component-based architecture supports modular, scalable UI development.

2. **Implement Authentication UI**:
   - **GCP Service**: Firebase Authentication
   - **Steps**:
     - Install Firebase: `npm install firebase @angular/fire`.
     - Configure Firebase in `environment.ts` with your Firebase project credentials.
     - Create a login component: `ng generate component login`.
     - Use Firebase Authentication SDK to implement Google SSO login.
     - Store the user’s JWT token in local storage for API requests.
   - **Why**: Firebase Authentication provides secure, enterprise-grade SSO (e.g., Google SSO), which is user-friendly for employees. It integrates seamlessly with Angular via `@angular/fire`.

3. **Develop Search Interface**:
   - **Tool**: Angular Material
   - **Steps**:
     - Install Angular Material: `ng add @angular/material`.
     - Create a search component: `ng generate component search`.
     - Use Angular Material components (e.g., `mat-form-field`, `mat-input`) for the search bar.
     - Display results using `mat-card` and `mat-list` for document snippets and answers from the Foundational Model.
     - Add metadata display (e.g., last modified date) using `mat-chip`.
   - **Why**: Angular Material provides pre-built, responsive UI components, ensuring a consistent and accessible user experience. It speeds up development with reusable components.

4. **Integrate with Backend**:
   - **Tool**: Angular HTTP Client
   - **Steps**:
     - Create a service for API calls: `ng generate service api`.
     - Use `HttpClient` to send search queries to the FastAPI backend with the user’s JWT token in headers.
     - Handle responses to display search results and answers generated by the Foundational Model.
     - Implement error handling for failed API requests (e.g., 401 Unauthorized).
   - **Why**: Angular’s `HttpClient` provides a robust way to communicate with the backend via REST APIs, supporting headers for authentication and error handling.

5. **Handle Temporal Relevance**:
   - **Steps**:
     - In the search results component, sort or highlight documents based on backend-provided metadata (e.g., `last_modified`).
     - Use Angular pipes to format dates (e.g., `date:'medium'`).
   - **Why**: Angular’s templating and pipes make it easy to display and sort metadata, ensuring users see recent information first.

6. **Testing**:
   - **Tool**: Jasmine/Karma (built into Angular)
   - **Steps**:
     - Write unit tests for components and services (e.g., `search.component.spec.ts`).
     - Test API integration with mock responses using `HttpClientTestingModule`.
     - Run tests: `ng test`.
   - **Why**: Jasmine/Karma ensures the frontend is reliable and bug-free, especially for critical features like search and authentication.

7. **Deployment**:
   - **GCP Service**: Firebase Hosting
   - **Steps**:
     - Install Firebase CLI: `npm install -g firebase-tools`.
     - Build the Angular app: `ng build --prod`.
     - Deploy to Firebase Hosting: `firebase deploy`.
   - **Why**: Firebase Hosting offers a simple, scalable way to deploy Angular apps with automatic SSL, CDN support, and easy rollbacks.

---

## Component 2: FastAPI Backend

### Purpose
The FastAPI backend handles search requests, integrates with external services (Confluence, Google Drive, Slack), manages authentication, and orchestrates the RAG pipeline with the Foundational Model.

### Implementation Plan
1. **Set Up FastAPI Project**:
   - **Tool**: FastAPI, Uvicorn
   - **Steps**:
     - Create a Python virtual environment: `python -m venv venv`.
     - Activate it: `source venv/bin/activate` (Linux/Mac) or `venv\Scripts\activate` (Windows).
     - Install FastAPI and Uvicorn: `pip install fastapi uvicorn`.
     - Create a `main.py` file with a basic FastAPI app.
   - **Why**: FastAPI is a high-performance, async Python framework for REST APIs, ideal for handling search and indexing workloads. Uvicorn is an ASGI server for running FastAPI apps.

2. **Implement Authentication**:
   - **GCP Service**: Firebase Authentication
   - **Steps**:
     - Install Firebase Admin SDK: `pip install firebase-admin`.
     - Download your Firebase service account key and initialize it in `main.py`.
     - Create a middleware to verify JWT tokens in incoming requests.
     - Fetch user roles from Firestore or Firebase custom claims for RBAC.
   - **Why**: Firebase Authentication provides secure JWT verification, and its Admin SDK integrates easily with FastAPI. It supports RBAC by storing roles in Firestore or custom claims.

3. **Integrate with External APIs**:
   - **Tools**: `requests` library, API-specific libraries
   - **Steps**:
     - Install dependencies: `pip install requests atlassian-python-api slack_sdk google-api-python-client`.
     - Create modules for each source:
       - **Confluence**: Use `atlassian-python-api` to fetch pages and attachments via the Atlassian REST API.
       - **Google Drive**: Use `google-api-python-client` to fetch files (Docs, Sheets, PDFs).
       - **Slack**: Use `slack_sdk` to fetch messages, threads, and files.
     - Store API credentials in GCP Secret Manager and access them in the app.
   - **Why**: These libraries simplify API interactions. GCP Secret Manager securely stores credentials, reducing the risk of exposure.

4. **Document Processing and Indexing**:
   - **Tools**: `PyPDF2`, Google Cloud Document AI, `langchain`
   - **Steps**:
     - Extract text from documents:
       - Use `PyPDF2` for PDFs.
       - Use Google Cloud Document AI for advanced text extraction from complex documents.
     - Generate embeddings using the Foundational Model via Vertex AI (e.g., `text-embedding-gecko`).
     - Store embeddings and metadata (e.g., `document_id`, `source`, `permissions`, `last_modified`) in Firestore or Pinecone.
   - **Why**: `PyPDF2` and Document AI handle text extraction efficiently. `langchain` simplifies embedding generation, and the Foundational Model on Vertex AI provides high-quality embeddings. Firestore/Pinecone are scalable for vector storage and search.

5. **Integrate the Foundational Model for RAG**:
   - **GCP Service**: Vertex AI
   - **Steps**:
     - Enable Vertex AI API in the GCP Console.
     - Select a Foundational Model (e.g., PaLM 2) for embeddings and natural language generation.
     - Install Vertex AI SDK: `pip install google-cloud-aiplatform`.
     - Configure FastAPI to use the Foundational Model:
       - For embeddings: Use Vertex AI’s endpoint to generate embeddings for documents and queries.
       - For natural language generation: Pass retrieved documents to the Foundational Model to generate answers.
   - **Why**: The Foundational Model, hosted on Vertex AI, provides state-of-the-art NLP capabilities for embeddings and response generation, ensuring accurate and coherent RAG outputs.

6. **Implement RAG Pipeline**:
   - **Steps**:
     - Create a `/search` endpoint in FastAPI:
       1. Receive the user query and JWT token.
       2. Verify the token and fetch user permissions.
       3. Generate query embedding using the Foundational Model (Vertex AI).
       4. Query the vector DB (Firestore/Pinecone) for top-k documents, filtering by permissions.
       5. Apply a recency boost: `score = semantic_similarity + recency_weight * (current_timestamp - last_modified)`.
       6. Pass top-k documents to the Foundational Model for a natural language answer.
       7. Return document snippets and the answer to the frontend.
   - **Why**: The Foundational Model ensures high-quality semantic search and natural language responses. The recency boost ensures temporal relevance.

7. **Schedule Periodic Indexing**:
   - **GCP Service**: Cloud Scheduler
   - **Steps**:
     - Create a Cloud Scheduler job to trigger the indexing endpoint nightly.
     - Implement an `/index` endpoint in FastAPI to fetch and process new/updated documents.
     - Use Pub/Sub to trigger the endpoint: `gcloud scheduler jobs create http index-job --schedule="0 0 * * *" --uri="https://your-backend-url/index"`.
   - **Why**: Cloud Scheduler automates indexing, ensuring the vector DB stays up-to-date with recent documents.

8. **Testing**:
   - **Tool**: Pytest
   - **Steps**:
     - Install Pytest: `pip install pytest pytest-asyncio`.
     - Write tests for API endpoints (e.g., `/search`, `/index`).
     - Mock external API calls, GCP services, and Foundational Model responses using `unittest.mock`.
     - Run tests: `pytest`.
   - **Why**: Pytest ensures the backend is reliable, especially for critical functions like search, indexing, and Foundational Model integration.

9. **Deployment**:
   - **GCP Service**: Cloud Run
   - **Steps**:
     - Containerize the app using Docker:
       - Create a `Dockerfile`:
         ```dockerfile
         FROM python:3.9-slim
         WORKDIR /app
         COPY . .
         RUN pip install -r requirements.txt
         CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8080"]
         ```
       - Build and push the image: `docker build -t gcr.io/your-project-id/knowledge-assistant-backend .` and `docker push gcr.io/your-project-id/knowledge-assistant-backend`.
     - Deploy to Cloud Run: `gcloud run deploy --image gcr.io/your-project-id/knowledge-assistant-backend --platform managed`.
   - **Why**: Cloud Run offers serverless deployment, auto-scaling, and cost efficiency for variable traffic.

---

## Component 3: GCP Infrastructure

### Purpose
GCP infrastructure provides scalable, secure services for hosting, storage, authentication, vector storage, Foundational Model inference, and scheduling.

### Implementation Plan
1. **Cloud Run (Hosting FastAPI)**:
   - **Steps**:
     - Already covered in the FastAPI deployment section.
     - Configure auto-scaling: Set min/max instances in Cloud Run settings.
     - Enable logging: Integrate with Cloud Logging for monitoring.
   - **Why**: Cloud Run is serverless, auto-scales for RAG and indexing workloads, and is cost-effective for variable traffic.

2. **Firebase Authentication (User Authentication and RBAC)**:
   - **Steps**:
     - Set up a Firebase project in the GCP Console.
     - Enable Google SSO as an authentication provider.
     - Store user roles in Firestore or Firebase custom claims:
       - Create a Firestore collection `users` with documents containing `user_id` and `role`.
       - Alternatively, set custom claims via Firebase Admin SDK: `admin.auth().setCustomUserClaims(uid, { role: 'employee' })`.
     - Integrate with FastAPI and Angular as described above.
   - **Why**: Firebase provides secure authentication with Google SSO, integrates with Angular and FastAPI, and supports RBAC via Firestore or custom claims.

3. **Firestore (Vector DB or Metadata Storage)**:
   - **Steps**:
     - Create a Firestore database in the GCP Console.
     - Set up collections for documents: `documents` (metadata) and `embeddings` (vector data, if not using Pinecone).
     - Configure security rules to enforce RBAC:
       ```firestore
       rules_version = '2';
       service cloud.firestore {
         match /databases/{database}/documents {
           match /documents/{documentId} {
             allow read: if request.auth != null && resource.data.permissions[request.auth.uid] == true;
           }
         }
       }
       ```
     - Index metadata fields (e.g., `last_modified`) for faster queries.
   - **Why**: Firestore is fully managed, scalable, and integrates with Firebase/GCP. It supports fast metadata queries for RBAC and recency.

4. **Pinecone (Optional Vector DB)**:
   - **Steps**:
     - Sign up for Pinecone and create an index: `knowledge-assistant-index`.
     - Install Pinecone client: `pip install pinecone-client`.
     - In FastAPI, upsert embeddings to Pinecone with metadata (e.g., `document_id`, `permissions`).
     - Query Pinecone for vector similarity search, filtering by permissions.
   - **Why**: Pinecone is purpose-built for vector search, offering better performance than Firestore for large-scale semantic search.

5. **Vertex AI with Foundational Model (Embeddings and Natural Language Generation)**:
   - **Steps**:
     - Enable Vertex AI API in the GCP Console.
     - Select a Foundational Model (e.g., PaLM 2) for embeddings and LLM inference.
     - Configure access to the model via Vertex AI endpoints.
     - Integrate with FastAPI as described in the RAG pipeline.
   - **Why**: Vertex AI hosts the Foundational Model, providing enterprise-grade NLP capabilities for embeddings and natural language generation, ensuring high-quality RAG results.

6. **Cloud Storage (Document Storage)**:
   - **Steps**:
     - Create a bucket: `gsutil mb gs://knowledge-assistant-documents`.
     - Configure lifecycle rules to manage storage costs (e.g., delete files older than 90 days).
     - In FastAPI, upload raw documents from Google Drive/Confluence to Cloud Storage.
   - **Why**: Cloud Storage is scalable, cost-effective, and integrates with Document AI for processing.

7. **Cloud Scheduler (Indexing Automation)**:
   - **Steps**:
     - Already covered in the FastAPI indexing section.
     - Monitor job execution in the GCP Console.
   - **Why**: Cloud Scheduler ensures automated, reliable indexing of recent documents.

8. **Secret Manager (Credential Management)**:
   - **Steps**:
     - Store API keys: `gcloud secrets create confluence-api-key --data-file=api_key.txt`.
     - Access secrets in FastAPI: Use `google-cloud-secret-manager` to retrieve credentials.
   - **Why**: Secret Manager securely stores sensitive data, integrating seamlessly with FastAPI and Cloud Run.

9. **Monitoring and Logging**:
   - **GCP Service**: Cloud Monitoring/Logging
   - **Steps**:
     - Enable Cloud Logging for Cloud Run and Firebase Hosting.
     - Set up Cloud Monitoring dashboards to track API latency, error rates, and Foundational Model inference performance.
     - Create alerts for critical issues (e.g., high error rates).
   - **Why**: Cloud Monitoring/Logging provides visibility into system performance and errors, enabling proactive maintenance.

---

## Additional Considerations

- **Scalability**:
  - Cloud Run and Firestore/Pinecone auto-scale to handle increased load.
  - Monitor Foundational Model inference costs on Vertex AI.
- **Cost Optimization**:
  - Use GCP cost calculator to estimate expenses.
  - Optimize indexing frequency and Foundational Model usage to reduce costs.
- **Error Handling**:
  - Implement retry logic in FastAPI for external API and Foundational Model failures.
  - Log errors to Cloud Logging for debugging.
- **Extensibility**:
  - Add new sources (e.g., Jira) by extending FastAPI modules.
  - Update the vector DB schema to accommodate new metadata fields.

---

## Timeline

- **Week 1-2**: Set up Angular and FastAPI projects, integrate Firebase Authentication.
- **Week 3-4**: Implement search UI, API integrations, and document processing.
- **Week 5-6**: Build RAG pipeline with Foundational Model, set up vector DB, and schedule indexing.
- **Week 7**: Test frontend and backend, deploy to Firebase Hosting and Cloud Run.
- **Week 8**: Monitor performance, optimize, and document for handover.

---

This implementation plan ensures a robust, scalable Internal Knowledge Assistant that meets all specified requirements, leveraging GCP services and a Foundational Model for efficiency and accuracy.